\documentclass[letterpaper, 12pt]{article}

% Imports appropriate packages
\usepackage{
  amsmath, amsthm, amssymb, mathtools, dsfont,
  graphicx, wrapfig, subfig, float, 
  listings, color, inconsolata, pythonhighlight,     
  fancyhdr, sectsty, hyperref, enumerate, enumitem, 
  rotating, physics }
\usepackage[mathscr]{euscript}
\usepackage[bottom]{footmisc}
\usepackage[left=1.35in, right=1.35in, bottom=1in, top=1.1in, headsep=0.2in]{geometry}
\usepackage[framemethod=TikZ]{mdframed}

% Set default enumerate
\setlist[enumerate,1]{label={(\alph*)}}

% Sets margins
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-0.5in}
\setlength{\itemindent}{-0.5in}

\let\b\mathbf
\let\bg\boldsymbol
\let\mc\mathcal

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% Title page
\title{\bf Final \\[2ex] 
       \rm\normalsize MATH 1620 --- Spring 2022}
\author{\today} %\author{\bf Madhav Ramesh}
\date{}

% Header and footer for other pages
\chead{Final}
\rhead{\thepage}
\cfoot{}

%---- MACROS ----%
\pagestyle{fancy}
\renewcommand{\footrulewidth}{1pt}
\renewcommand{\headrulewidth}{1.4pt}

\newcommand{\bias}{\textnormal{bias}}
\newcommand{\se}{\textnormal{se}}
\newcommand{\vari}{\textnormal{var}}
\newcommand{\cov}{\textnormal{cov}}
\newcommand{\mse}{\textnormal{mse}}
\newcommand{\E}{\mathbb{E}}

% CHANGE MATRIX SPACING
\makeatletter
\renewcommand*\env@matrix[1][\arraystretch]{%
  \edef\arraystretch{#1}%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols c}}


\makeatother

% PROBLEM ENVIRONMENT
\newcounter{prob}[section]\setcounter{prob}{0}
\renewcommand{\theprob}{\arabic{prob}}
\newenvironment{prob}[2][]{%
\refstepcounter{prob}%
\ifstrempty{#1}%
{\mdfsetup{%
frametitle={%
\tikz[baseline=(current bounding box.east),outer sep=0pt]
\node[anchor=east,rectangle,fill=black!20]
{\strut Problem~\theprob};}}
}%
{\mdfsetup{%
frametitle={%
\tikz[baseline=(current bounding box.east),outer sep=0pt]
\node[anchor=east,rectangle,fill=black!20]
{\strut ~#1};}}%
}%
\mdfsetup{innertopmargin=4pt,innerbottommargin=8pt,linecolor=black!20,%
linewidth=2pt,topline=true,%
frametitleaboveskip=\dimexpr-\ht\strutbox\relax
}
\begin{mdframed}[]\relax%
\label{#2}}{\end{mdframed}}

% CODING ENVIRONMENT
\RequirePackage{listings}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\definecolor{Darkgreen}{rgb}{0,0.4,0}
\lstset{
    backgroundcolor=\color{lbcolor},
    tabsize=4,
%   rulecolor=,
    language=Python,
        mathescape=true,
        basicstyle=\scriptsize,
        upquote=true,
        aboveskip={1.5\baselineskip},
        columns=fixed,
        showstringspaces=false,
        extendedchars=false,
        breaklines=true,
        prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
        frame=single,
        numbers=left,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.026,0.112,0.095},
        stringstyle=\color[rgb]{0.627,0.126,0.941},
        numberstyle=\color[rgb]{0.205, 0.142, 0.73},
}
%---- DOCUMENT ----%
\begin{document}
\maketitle
\thispagestyle{empty}

\begin{prob}[Problem 1]{}
    Prove that $kF_{k, \nu, \alpha}$ is increasing in $k$, and deduce as a consequence that 
    $$t_{\nu, \alpha/2} < \sqrt{(k - 1)F_{k - 1, \nu, \alpha}$$ 
    Thus the Scheffe confidence intervals are always wider than the confidence intervals for a
    single contrast.
    %Try an inductive proof based on the fact that you can write F random variable in terms of
    %chi-square variables. THen, you get an ordered relationship between the cdfs that implies
    %ordered relationship between inverse cdfs. 
\end{prob}
\begin{proof}
    Let $W_k$ be $F$ distributed with $df1 = k$ and $df2 = \nu$. We first show that 
       $$P\left(W_k \leq \frac{q}{k}\right) \leq P\left(W_{k - 1} \leq \frac{q}{k - 1}\right)$$ 
    Note that we can rewrite $W_k$ as 
    $$W_k = \frac{(Z_1^2 + \cdots + Z_k^2)/k}{V/\nu}$$ 
    where $Z_1, \ldots, Z_k$ and $V$ are all independent. Each $Z_j \sim \textnormal{norm}(0, 1)$
    for $1 \leq j \leq n$ and $V \sim \textnormal{chisq}(df = \nu)$. Plugging in for $W_k$, we want
    to show
    \begin{align*}
        P\left(\frac{(Z_1^2 + \cdots + Z_{k - 1}^2 + Z_k^2)/k}{V/\nu} \leq \frac{q}{k}\right) &\leq 
        P\left(\frac{(Z_1^2 + \cdots + Z_{k - 1}^2)/(k - 1)}{V/\nu} \leq \frac{q}{k - 1}\right) \\ 
        P\left(\frac{(Z_1^2 + \cdots + Z_{k - 1}^2 + Z_k^2)}{V/\nu} \leq q\right) &\leq 
        P\left(\frac{(Z_1^2 + \cdots + Z_{k - 1}^2)}{V/\nu} \leq q\right) \\ 
        P\left(Z_1^2 + \cdots + Z_{k - 1}^2 + Z_k^2 \leq q\right) &\leq 
        P\left(Z_1^2 + \cdots + Z_{k - 1}^2 \leq q\right)
    \end{align*}
    The only difference in the two expressions is that on the left side, we have one additional
    positive random variable: $Z_k^2$. However, since this random variable is always positive,
    adding it will only decrease the probability of being smaller than $q$. Thus, we have proven
    that 
       $$P\left(W_k \leq \frac{q}{k}\right) \leq P\left(W_{k - 1} \leq \frac{q}{k - 1}\right)$$ 
    The cdf and the quantile function have inverse relationships. Thus, if the cdf for $W_k$ is less
    than or equal to the cdf for $W_{k - 1}$, the quantile function for $W_k$ must be greater than
    the quantile function for $W_{k - 1}$. Thus, we can write 
    $$\verb|kqf(p, df1 = k)| > \verb|(k - 1)qf(p, df2 = k - 1)|$$ 
    where $p = 1 - \alpha$. Since this holds for every $k$, we conclude that $kF_{k,\nu,\alpha}$ 
    is increasing. We now show
    that as a consequence, 
    $$t_{\nu, \alpha/2} < \sqrt{(k - 1)F_{k - 1, \nu, \alpha}}$$ 
    Recall that $t_{\nu, \alpha/2} = \sqrt{F_{1, \nu, \alpha}}$. However, we just showed that 
    $F_{1, \nu, \alpha} < (k - 1)F_{k,\nu,\alpha}$. Thus, 
    $$t_{\nu, \alpha/2} < \sqrt{(k - 1)F_{k, \nu, \alpha}}$$
\end{proof}

\begin{prob}[Problem 2]{}
    Set up an ANOVA table, that includes the ANOVA F statistic $p$-value, with the fist toxin data
    in C\&B: 
    $$\begin{tabular}{c c c c}
        toxin 1 & toxin 2 & toxin 3 & control \\ 
        \hline 
        28 & 33 & 18 & 11 \\ 
        23 & 36 & 21 & 14 \\ 
        14 & 34 & 20 & 11 \\ 
        27 & 29 & 22 & 16 \\ 
           & 31 & 24 & \\ 
           & 34 & & 
    \end{tabular}$$
    Find the $p$-value for each of the individual comparison $\theta_a = \theta_b$ for $a \neq b$
\end{prob}
\begin{proof}[Solution]
    The figure below shows the ANOVA table for the fish toxin data. 
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{../images/final_2.png}
    \end{figure}
    The p-values for comparisons of the form $\theta_a = \theta_b$ for $a \neq b$ are shown
    below. 
    \begin{align*}
        \theta_0 &= \theta_1: 0.00033 \\ 
        \theta_0 &= \theta_2: 0.20817 \\ 
        \theta_0 &= \theta_3: 0.00062 \\
        \theta_1 &= \theta_2: 3.1762e-05 \\ 
        \theta_1 &= \theta_3: 1.7112e-05 \\
        \theta_2 &= \theta_3: 0.00222
    \end{align*}
\end{proof}

\begin{prob}[Problem 3]{}
    In 1973, the President of Texaco, Inc., made a statement to a U.S. Senate
subcommittee concerned with air and water pollution. The committee was concerned
with, among other things, the noise levels associated with automobile filters. He cited
the data in the table below from a study that included vehicles of three different sizes.
    $$\begin{tabular}{c c c}
        small & medium & large \\ 
        \hline 
        810 & 840 & 785 \\ 
        820 & 840 & 790 \\ 
        820 & 840 & 785 \\ 
        835 & 845 & 760 \\ 
        835 & 855 & 760 \\ 
        835 & 850 & 770
    \end{tabular}$$
    Construct an ANOVA table for these data, giving the $p$-value for the ANOVA $F$ statistic. Find
    the $p$-value for each of the individual comparison $\theta_a = \theta_b$ for $a \neq b$. 
\end{prob}
\begin{proof}[Solution]
   The figure below shows the ANOVA table for the noise data. 
   \begin{figure}[H]
       \centering
       \includegraphics[width=0.8\textwidth]{../images/final_3.png}
   \end{figure}
    The p-values for comparisons of the form $\theta_a = \theta_b$ for $a \neq b$ are shown
    below. 
    \begin{align*}
        \theta_0 &= \theta_1: 0.00333 \\ 
        \theta_0 &= \theta_2: 2.5601e-07 \\
        \theta_1 &= \theta_2: 3.9139e-09
    \end{align*}
\end{proof}

\begin{prob}[Problem 4]{}
    Note that in an ANOVA with k treatments, there are ${k \choose 2} = \frac{k(k - 1)}{2}$ 
    pairs of means to compare. A multiple comparison procedure called
    the \textit{Protected LSD} has the goal of determining which pairs of means are different,
    while controlling the experimentwise error rate, and is performed as follows. First
    apply the ANOVA F test at level $\alpha$. (The null hypothesis of this test is that all means
    are equal). 
    \begin{itemize}
        \item If the result of the ANOVA F test at level $\alpha$ is that the ANOVA null should be
            retained (the p-value is > $\alpha$), then declare that ``all means are equal'' and stop (do
            not make pairwise comparisons) 
        \item  If the result of the ANOVA F test at level $\alpha$ is that the ANOVA null is rejected
(the p-value is $\leq \alpha$), then proceed to inspect pairwise differences as follows:
    \end{$$itemize}
    For each $1 \leq a \neq b \leq k$, declare the means $\theta_a$ and $\theta_b$ to be different
    if 
    $$\frac{\|\overline{Y}_{a\cdot} - \overline{Y}_{b\cdot}\|}{S_p\sqrt{\frac{1}{n_a} +
            \frac{1}{n_b}}} > t_{n - k, \alpha/2}$$ 
    \begin{enumerate}
        \item Explain that no matter how many means are compared (how large $k$ is ) the Protected
            LSD test is made at level $\alpha$ (the experimentwise Type I error is $\leq \alpha$) 
        \item Perform the test on the fish toxin data (Exercise 2)
    \end{enumerate}
\end{prob}
\begin{proof}
    \begin{enumerate}
        \item Observe that if all the means are equal (the null hypothesis is true), then the ANOVA
            F test rejects the null hypothesis $\alpha$ of the time. This is because the Type I error 
            rate of the ANOVA F test is $\alpha$. Thus, pairwise differences are inspected only
            $\alpha$ of the time. This exactly means that the experimentwise Type I error
            remains the same.
        \item From Exercise (2), we know that the ANOVA F test at level $\alpha$ has a p-value of
            0.000003. Even for a choice of $alpha = 0.01$, we have $0.000003 < \alpha$ so the result
            of the ANOVA F test is that the ANOVA null should be rejected. Thus, we inspect the
            pairwise differences using the protected LSD test. Below, we list the statistics 
            obtained for each $1 \leq a \neq b \leq k$. 
            \begin{align*}
                \theta_0 = \theta_1: 4.2710 \\ 
                \theta_0 = \theta_2: 0.8359 \\ 
                \theta_0 = \theta_3: 3.9649 \\ 
                \theta_1 = \theta_2: 5.4789 \\ 
                \theta_1 = \theta_3: 8.6143 \\ 
                \theta_2 = \theta_3: 3.3435
            \end{align*}
            Note that if we were using $\alpha = 0.05$, then $t_{n - k, \alpha/2} = 2.1314$. Since
            all of the statistics obtained are greater than 2.1314, we would reject the null
            hypothesis and conclude that $\theta_a \neq \theta_b$ for all $1 \leq a \neq b \leq k$. 
    \end{enumerate}
\end{proof}

\begin{prob}[Problem 5]{}
    Apply Tukey’s method on the Texaco data in Exercise 3.
\end{prob}
\begin{proof}[Solution]
    Using Tukey's method on the Texaco data, we get $Q = 11.4878$. For $\alpha = 0.05$, we get 
    $$\verb|qtukey|(1 - \alpha, 3, 15) = 3.67$$ 
    Since $Q > 3.67$, we can reject the null hypothesis.
\end{proof}

\begin{prob}[Problem 6]{}
    Suppose that $Y$ belongs to a family of distributions with mean $\theta$ and variance
    $v(\theta)$. 
    \begin{enumerate}
        \item For a smooth function $g : \R \to \R$ with $g'(y) > 0$, show by Taylor approximation
            that the approximate variance of $g(Y)$ is 
            $$\vari g(Y) = g'(\theta)^2 v(\theta)$$ 
        \item Show that if we take 
            $$g^*(y) - \int v(y)^{-1/2} dy$$ 
            (take $g^*(y)$ to be an antiderivative of $v(y)^{-1/2}$) then $g^*(Y)$ has variance
            approximately independent of $\theta$. 
        \item Show that the following transformation is approximately variance-stabilizing: 
            $$Y \sim \textnormal{binom}(n, p), \qquad g^*(y) = sin^{-1}(\sqrt{y / n})$$ 
        \item Show that the following transformation is approximately variance-stabilizing: 
            $$Y \sim \textnormal{pois}(\lambda), \qquad g^*(y) = \sqrt{y}$$ 
        \item The Box-Cox family of power transformations are given by 
            $$\begin{cases}
                \frac{y^\lambda - 1}{\lambda} & \textnormal{if $\lambda \neq 0$} \\ 
                \log y & \textnormal{if $\lambda = 0$}
            \end{cases}$$
            Find the function $v(\theta)$ that $g_{\lambda}^*(y)$ stabilizes.
    \end{enumerate}
\end{prob}
\begin{proof}[Solution]
    \begin{enumerate}
        \item Recall that we can use a Taylor approximation to write 
            $$\vari (g(Y)) \approx g'(E(Y))^2 \vari(Y) = g'(\theta)^2 v(\theta)$$ 
        \item Let $g(y) = g^*(y)$. Then, using (a), we have 
            \begin{align*}
                \vari(g(y)) &= g'(g(\theta))^2 v(\theta) \\ 
                              &= \left(\frac{d}{d\theta}\int_{-\infty}^\theta v(y)^{-1/2}
                              dy\right)^2 v(\theta) \\ 
                              &= (v(\theta)^{-1/2})^2v(\theta) \\ 
                              &= 1
            \end{align*}
            Clearly, $\vari(g^*(y))$ is approximately independent of $\theta$. 
        \item Let $g(y) = g^*(y)$ so $g(\theta) = g^*(\theta)$. 
            Note that 
            \begin{align*}
                \theta &= E(Y) = np \\ 
                v(\theta) &= \vari(Y) = np(1 - p) = \theta\left(1 - \frac{\theta}{n}\right)
            \end{align*} 
            Also, 
            \begin{align*}
                g'(\theta)^2 &= \left(\frac{1}{\sqrt{1 - \frac{\theta}{n}}}\cdot
                    \frac{1}{2\sqrt{\frac{\theta}{n}}}\cdot\frac{1}{n}\right)^2 \\ 
                      &= \frac{1}{1 -
                          \frac{\theta}{n}}\cdot\frac{1}{\frac{\theta}{n}}\cdot\frac{1}{4n^2} \\ 
                      &= \frac{1}{4n\theta(1 - \frac{\theta}{n})}
            \end{align*}
            Using (a), 
            \begin{align*}
                \vari(g(y)) &= g'(\theta)^2 v(\theta) \\ 
                              &= \frac{1}{4n\theta(1 - \frac{\theta}{n})} \cdot \theta\left(1 -
                                  \frac{\theta}{n}\right) \\ 
                              &= \frac{1}{4n}
            \end{align*}
            Thus, $g^*(y)$ has variance approximately independent of $\theta$ and the given
            transformation is approximately variance-stabilizing.
        \item Let $g(y) = g^*(y)$. 
            Note that $\theta = E(Y) = \lambda$ and $v(\lambda) = \vari(Y) = \lambda$. Also,
            $$g'(\lambda)^2 = \left(\frac{1}{2\sqrt{\lambda}}\right)^2 = \frac{1}{4\lambda}$$ 
            Using (a), 
            \begin{align*}
                \vari(g(y)) &= g'(\lambda)^2 v(\lambda) \\ 
                              &= \frac{1}{4\lambda} \cdot \lambda \\ 
                              &= \frac{1}{4}
            \end{align*}
            Thus, $g^*(y)$ has variance approximately independent of $\lambda$ and the given
            transformation is approximately variance-stabilizing.
        \item We first note that 
            $$\lim_{\lambda \to 0} \frac{y^{\lambda} - 1}{\lambda} = \lim_{\lambda \to 0} (\log
            y)y^{\lambda} = \log y$$ 
            so $g_\lambda^*(y)$ is continuous.
            From (b), we know 
            $$g^*(y) = \int v(y)^{-1/2} dy$$ 
            Since $g_\lambda^*(y)$ is continuous, we can plug in to get
            \begin{align*}
                \frac{y^{\lambda} - 1}{\lambda} = \int v(y)^{-1/2} dy 
            \end{align*}
            We now solve for $v(y)$, beginning by taking the derivative of both sides with respect
            to y.
            \begin{align*}
                y^{\lambda - 1} &= v(y)^{-1/2} \\ 
                v(y) &= y^{-2(\lambda - 1)}
            \end{align*}
            Thu,s $v(\theta) = \theta^{-2(\lambda - 1)}$. We can confirm that this transformation is
            approximately variance-stabilizing using the same methods as (c) and (d). We know that
            $g'(\theta)^2 = \theta^{2(\lambda - 1)}$. Thus, 
            \begin{align*}
                \vari(g_\lambda^*(y)) &= \theta^{2(\lambda - 1)} \cdot \theta^{-2(\lambda - 1)} \\ 
                                      &= 1
            \end{align*}
    \end{enumerate}
\end{proof}

\begin{prob}[Problem 7]{}
    Consider the following data 
    $$\begin{tabular}{c c c}
        $i$ & $x_i$ & $y_i$ \\ 
        \hline 
        1 & 0.3 & 0.4 \\
        2 & 1.4 & 0.9 \\ 
        3 & 1.0 & 0.4 \\ 
        4 & -0.3 & -0.3 \\ 
        5 & -0.2 & 0.3 \\ 
        6 & 1.0 & 0.8 \\ 
        7 & 2.0 & 0.7 \\ 
        8 & -1.0 & -0.4 \\ 
        9 & -0.7 & -0.2 \\ 
        10 & 0.7 & 0.7
    \end{tabular}$$
    Apply simply linear regression where it is assumed that the response 
    $$Y | x \sim \textnormal{norm}(\alpha + \beta x, \sigma^2)$$ 
    (the conditional normal model) 
    \begin{enumerate}
        \item Test at level $\alpha = 0.05$ the hypothesis that the regression line is horizontal. 
        \itme Test at level $\alpha = 0.10$ the hypothesis 
        $$H_0 : \beta = 5\alpha, \qquad \textnormal{versus} \qquad H_1 : \beta \neq 5 \alpha$$ 
    \item Test at level $\alpha = 0.01$ the hypothesis that when $x = 1$, the height of the
        regression line is $y = 1$. 
    \item Sketch a confidence band in the $xy$-plane for the regression line with confidence
        coefficient 0.95 
    \item On the same graph, sketch the curves which specify the $y$-limits at each point $x$ of a
        confidence interval with confidence coefficient 0.95 for the value of the regression line at
        the poitn $x$. 
    \end{enumerate}
\end{prob}
\begin{proof}[Solution]
    \begin{enumerate}
        \item We wish to test the hypothesis 
            $$H_0: \beta = 0, \qquad \textnormal{versus} \qquad H_1: \beta \neq 0$$ 
            We begin by calculating $\hat{\beta}, \hat{\alpha}$, and $\hat{\sigma^2}$. These are
            shown in a table below.
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.5\textwidth]{../images/final_7.png}
            \end{figure}
            We know that 
            $$T = \frac{\hat{\beta} - \beta}{S/\sqrt{s_{xx}}} \sim t(n - 2)$$ 
            Plugging the appropriate values into the equation above gives $T = 5.3126$. Since
            $5.3126 > t_{n - 2, 0.975} = 2.306$, we reject the null hypothesis. Thus, we conclude that 
            the regression line is not horizontal.
        \item We rewrite the hypothesis test as 
            $$H_0: \alpha - \frac{1}{5}\beta = 0, \qquad \textnormal{versus} \qquad H_1: \alpha -
            \frac{1}{5}\beta \neq 0$$ 
            We know that 
            $$T = \frac{(\hat{\alpha} - \hat{\beta}x_0) - (\alpha + \beta
            x_0)}{\sigma\sqrt{\frac{1}{n} + \frac{(x_0 - \overline{x})^2}{s_{xx}}}} \sim t(n - 2)$$ 
            Plugging the appropriate values into the equation above with $x_0 = -\frac{1}{5}$ and
            $\alpha + \beta x_0 = 0$ gives $T = 0.6639$. Since $0.6639 < t_{n - 2, 0.95} = 1.8595$,
            we retain the null hypothesis. Thus, we conclude that $\alpha - \frac{1}{5}\beta = 0$ or
            $\beta = 5\alpha$.
        \item We know that 
            $$T = \frac{(\hat{\alpha} + \hat{\beta}x_0) - (\alpha + \beta x_0)}{S\sqrt{\frac{1}{n} +
            \frac{(x_0 - \overline{x})^2}{s_{xx}}}} \sim t(n - 2)$$ 
            Plugging $x_0 = 1$ and $\alpha + \beta x_0 = 1$ in along with the previously calculated
            MLEs gives $T = 4.701$. Since $4.701 > t_{n - 2, 0.995} = 3.355$, we reject the null
            hypothesis. Thus, we conclude that when $x = 1$, the height of the regression line is
            not $y = 1$. 
        \item The sketch of the 95\% Scheffe bands are shown below (blue line) along with the curves
            which specify the $y$-limits at each point $x$ of a confidence interval with confidence
            coefficient 0.95 for the value of the regression line at point $x$ (red line).
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.6\textwidth]{../images/final_7d.png}
            \end{figure}
        \item Shown in figure above.
    \end{enumerate}
\end{proof}

\newpage
\section*{Appendix}
\begin{lstlisting}
import dataframe_image as dfi
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.stats as stats


def calculate_p_values(df, a):
    num = (a * df.mean()).sum()

    degrees = df.count().sum() - len(df.columns)
    sp = np.sqrt((1 / degrees) * ((df - df.mean()) ** 2).sum().sum())
    denom = sp * np.sqrt((a ** 2 / df.count()).sum())

    p_value = stats.t.sf(abs(num / denom), degrees)
    return p_value


def anova(df):
    df_between = len(df.columns) - 1
    df_within = df.count().sum() - df_between - 1
    total = df_between + df_within

    ssb = (df.count() * ((df.mean() - df.stack().mean()) ** 2)).sum()
    ssw = ((df - df.mean()) ** 2).sum().sum()
    sst = ((df - df.stack().mean()) ** 2).sum().sum()

    msb = ssb / df_between
    msw = ssw / df_within
    f_statistic = msb / msw
    p_value = stats.f.sf(abs(f_statistic), df_between, df_within)

    results_data = {
        "Degrees of freedom": [df_between, df_within, total],
        "Sum of squares": [ssb, ssw, sst],
        "Mean square": [msb, msw, np.nan],
        "F statistic": [f_statistic, np.nan, np.nan],
        "F statistic p-value: ": [p_value, np.nan, np.nan],
    }
    results_dataframe = pd.DataFrame(data=results_data)
    results_dataframe.index = [
        "Between treatment groups",
        "Within treatment groups",
        "Total",
    ]

    print("Initial Dataframe: ")
    print(df)
    print()
    print("ANOVA Table: ")
    print(results_dataframe)

    for i in range(len(df.columns) - 1):
        for j in range(i + 1, len(df.columns)):
            a = np.zeros(len(df.columns))
            a[i] = 1
            a[j] = -1
            print(
                f"p_value for theta_{i} = theta_{j}: ",
                calculate_p_values(df, a),
            )

    return results_dataframe


def exercise2():
    toxin_data = {
        "toxin 1": [28, 23, 14, 27, np.nan, np.nan],
        "toxin 2": [33, 36, 34, 29, 31, 34],
        "toxin 3": [18, 21, 20, 22, 24, np.nan],
        "control": [11, 14, 11, 16, np.nan, np.nan],
    }
    toxin_dataframe = pd.DataFrame(data=toxin_data)

    results = anova(toxin_dataframe)
    dfi.export(results, "../images/final_2.png")


def exercise3():
    noise_data = {
        "small": [810, 820, 820, 835, 835, 835],
        "medium": [840, 840, 840, 845, 855, 850],
        "large": [785, 790, 785, 760, 760, 770],
    }
    noise_dataframe = pd.DataFrame(data=noise_data)

    results = anova(noise_dataframe)
    dfi.export(results, "../images/final_3.png")


def exercise4():
    # Protected LSD Test
    toxin_data = {
        "toxin 1": [28, 23, 14, 27, np.nan, np.nan],
        "toxin 2": [33, 36, 34, 29, 31, 34],
        "toxin 3": [18, 21, 20, 22, 24, np.nan],
        "control": [11, 14, 11, 16, np.nan, np.nan],
    }
    toxin_dataframe = pd.DataFrame(data=toxin_data)

    degrees = toxin_dataframe.count().sum() - len(toxin_dataframe.columns)
    sp = np.sqrt(
        (1 / degrees) * ((toxin_dataframe - toxin_dataframe.mean()) ** 2).sum().sum()
    )

    for i in range(len(toxin_dataframe.columns) - 1):
        for j in range(i + 1, len(toxin_dataframe.columns)):
            num = abs(
                toxin_dataframe.iloc[:, i].mean() - toxin_dataframe.iloc[:, j].mean()
            )
            denom = sp * np.sqrt(
                (1 / toxin_dataframe.iloc[:, i].count())
                + (1 / toxin_dataframe.iloc[:, j].count())
            )

            print(f"t-statistic for {i}-{j}", abs(num / denom))

    print("t-statistic corresponding to alpha: ", stats.t.ppf(0.975, degrees))


def exercise5():
    noise_data = {
        "small": [810, 820, 820, 835, 835, 835],
        "medium": [840, 840, 840, 845, 855, 850],
        "large": [785, 790, 785, 760, 760, 770],
    }
    noise_dataframe = pd.DataFrame(data=noise_data)

    degrees = noise_dataframe.count().sum() - len(noise_dataframe.columns)
    sp = np.sqrt(
        (1 / degrees) * ((noise_dataframe - noise_dataframe.mean()) ** 2).sum().sum()
    )
    num = noise_dataframe.mean().max() - noise_dataframe.mean().min()
    denom = sp * np.sqrt(2 / noise_dataframe["small"].count())

    print("Tukey's Method Sample Value Q: ", num / denom)


def exercise7():
    data = {
        "x": [0.3, 1.4, 1.0, -0.3, -0.2, 1.0, 2.0, -1.0, -0.7, 0.7],
        "y": [0.4, 0.9, 0.4, -0.3, 0.3, 0.8, 0.7, -0.4, -0.2, 0.7],
    }
    dataframe = pd.DataFrame(data=data)

    x_mean = dataframe["x"].mean()
    y_mean = dataframe["y"].mean()
    x_std = ((dataframe["x"] - x_mean) ** 2).sum()
    y_std = ((dataframe["y"] - y_mean) ** 2).sum()
    xy_std = ((dataframe["x"] - x_mean) * (dataframe["y"] - y_mean)).sum()

    n = dataframe["y"].count()
    beta_hat = xy_std / x_std
    alpha_hat = y_mean - (beta_hat * x_mean)
    sigma_hat_squared = (1 / n) * (y_std - (xy_std ** 2 / x_std))
    s = np.sqrt((n / (n - 2)) * sigma_hat_squared)

    results_data = {
        r"$\hat{\beta}$": [beta_hat],
        r"$\hat{\alpha}$": [alpha_hat],
        r"$\hat{\sigma^2}$": [sigma_hat_squared],
    }
    results_dataframe = pd.DataFrame(data=results_data)

    print("MLE Calculations: ")
    print(results_dataframe)
    dfi.export(results_dataframe, "../images/final_7.png")

    # 7a. Testing hypothesis that regression line is horizontal
    statistic = abs(beta_hat / (s / np.sqrt(x_std)))
    t_critical = stats.t.ppf(0.975, n - 2)
    print("Testing hypothesis that regression line is horizontal: ")
    print(f"T-statistic: {statistic}")
    print(f"T-critical value: {t_critical}")
    print()

    # 7b. Testing hypothesis that beta = 5 * alpha
    num = alpha_hat + (beta_hat * (-1 / 5))
    denom = s * np.sqrt((1 / n) + (((-1 / 5) - x_mean) ** 2 / x_std))
    statistic = abs(num / denom)
    t_critical = stats.t.ppf(0.95, n - 2)
    print("Testing hypothesis that beta = 5 * alpha")
    print(f"T-statistic: {statistic}")
    print(f"T-critical value: {t_critical}")
    print()

    # 7c. Testing hypothesis that when x = 1, height of regression line is y = 1
    num = (alpha_hat + beta_hat) - 1
    denom = s * np.sqrt((1 / n) + ((1 - x_mean) ** 2 / x_std))
    statistic = abs(num / denom)
    t_critical = stats.t.ppf(0.995, n - 2)
    print("Testing hypothesis that when x = 1, height of regression line is y = 1")
    print(f"T-statistic: {statistic}")
    print(f"T-critical value: {t_critical}")

    # 7d. Confidence band with confidence coefficient 0.95
    x_0 = np.linspace(0, 5, 50)
    t_val = stats.t.ppf(0.975, n - 2)
    lower_bound = (
        alpha_hat
        + (beta_hat * x_0)
        - (t_val * s * np.sqrt((1 / n) + ((x_0 - x_mean) ** 2 / x_std)))
    )
    upper_bound = (
        alpha_hat
        + (beta_hat * x_0)
        + (t_val * s * np.sqrt((1 / n) + ((x_0 - x_mean) ** 2 / x_std)))
    )
    line = alpha_hat + (beta_hat * x_0)

    # 7e. Adding curves for y-limits at each point x of a confidence interval with confidence
    # coefficient 0.95
    lower_prediction_interval = (
        alpha_hat
        + (beta_hat * x_0)
        - (t_val * s * np.sqrt(1 + (1 / n) + ((x_0 - x_mean) ** 2 / x_std)))
    )
    upper_prediction_interval = (
        alpha_hat
        + (beta_hat * x_0)
        + (t_val * s * np.sqrt(1 + (1 / n) + ((x_0 - x_mean) ** 2 / x_std)))
    )

    plt.plot(x_0, line, "-k", label=r"$\hat{\alpha} + \hat{\beta}x_0$")
    plt.plot(x_0, lower_bound, "--b", label=r"95% Scheffe bands")
    plt.plot(x_0, upper_bound, "--b")
    plt.plot(x_0, lower_prediction_interval, ":r", label=r"95% y-limit curves")
    plt.plot(x_0, upper_prediction_interval, ":r")
    plt.title("95% Scheffe bands and 95% curves for y-limits")
    plt.legend()
    #  plt.savefig("../images/final_7d.png")
    plt.show()


if __name__ == "__main__":
    print("-------------- Exercise 2 --------------")
    exercise2()
    print()

    print("-------------- Exercise 3 --------------")
    exercise3()
    print()

    print("-------------- Exercise 4 --------------")
    exercise4()
    print()

    print("-------------- Exercise 5 --------------")
    exercise5()
    print()

    print("-------------- Exercise 7 --------------")
    exercise7()
    print()
\end{lstlisting}

\end{document}
